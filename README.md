# Back-propagation-assignment
This tutorial provides a detailed explanation of Backpropagation and Gradient Descent, two fundamental concepts in neural networks . It includes both theoretical insights and practical implementations in Python.
The code and tutorial are simplified to help students practice and deepen their understanding of backpropagation.

Topics covered : 

Explaining the fundamentals of backpropagation and gradient descent through     relatable analogies to make learning easier for students.
Exploring how different factors, such as the optimizer, batch size, and learning rate, influence the outcomes.
How results can vary with different Gradients.
Step-by-Step Code Implementation using Python.
Visualizations to Illustrate How Gradients Are Computed.
Practical Examples on Optimizing a Simple Neural Network

Files in This Repository:
Jupyter Notebook with code and explanations
[MLassignment.ipynb] (https://colab.research.google.com/drive/1JcsKYUOm4sl4eITuDMzBXyS05KgVzt-w?usp=share_link) 

Backpropagation- ML assignment.pdf – A written guide explaining the concepts

README.md – This file with project details

LICENSE – This project is licensed under the MIT License. See the LICENSE file for details.

How to run the code ? 

Here is link to github:
https://github.com/sharonsibi07/Back-propagation-assignment.git 


Repository name:
Back-propagation-assignment

Jupyter notebook file:
https://colab.research.google.com/drive/1JcsKYUOm4sl4eITuDMzBXyS05KgVzt-w?usp=share_link 


